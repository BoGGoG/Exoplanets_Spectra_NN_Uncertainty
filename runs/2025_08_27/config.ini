
[GENERAL]
accelerator = cuda              # cpu or cuda, accelerator for lightning trainer
precision = 32            # precision for models (16-mixed, 32, 64), 16-mixed is recommended

[DIRECTORIES]
rundir = runs/2025_08_27      # directory where to store the results of this run


[MODEL]
model_class = CNNTimeSeriesRegressor # Model_03
max_norm_clipping = 1.0
; hyperparameter optimzation
n_events_hpopt = 5000
epochs_hpopt = 7
n_trials_hpopt = 400
timeout_hpopt = 200_000
swa_start_hpopt = 90
max_norm_clip = 1.0
alpha = 1.0  # factor in NLL loss
; ; retrain best model
use_pretrained = False
; use_pretrained = runs/2025_08_25/out/final/lightning_logs/cont_CNNTimeSeriesRegressor_2025-08-25/CNNTimeSeriesRegressor/version_2/checkpoints/epoch=145-step=618456-val_loss=0.00000.ckpt
compile = False
lr = 6e-4
q = 0.05                        # Only consider models in best q quantile. Take smallest model in this quantile for continuing training
epochs = 120                   # number of epochs to use for retraining of best model
swa_start = 90
swa_lr = 1e-6
n_events = -1
opset_version_onnx = 17         # onnx opset version for exporting the model

[ENSEMBLE]
; needs to first have done hpopt
; use_pretrained = False # give path to ckpt here. If so, this model will be used as base model for ensemble
use_pretrained = runs/2025_08_25/out/final/lightning_logs/cont_CNNTimeSeriesRegressor_2025-08-25/CNNTimeSeriesRegressor/version_2/checkpoints/epoch=145-step=618456-val_loss=0.00000.ckpt
ensemble_savedir = runs/2025_08_27/out/ensemble/ensemble_2
n_ensemble_models = 10               # number of models in ensemble
n_events = -1
epochs = 70
; batch_size = 128 # if not given, use the one from model
lr = 1e-3
swa_start = 60
swa_lr = 5e-5
